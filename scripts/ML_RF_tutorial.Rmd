---
title: "Omics ML tutorial"
author: "Julie Chih-yu Chen"
date: "04/05/2023"
output:
  html_document:
    toc: true
    toc_float: true
---

ML tutorial for the IMED7280 class at the University of Manitoba

The microbiome data and scripts were modified from previous work in Forbes, Chen et al. Microbiome 2018: https://pubmed.ncbi.nlm.nih.gov/30545401/


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(caret)
library(ROCR)
library(gplots)
library(ggfortify)
library(ggplot2)
```

## Loading data

```{r inputData}
## Load data in data folder
dat2use <- "../data/MicrobiomeData1.txt"
dat4train <- read.table(dat2use, header = T, sep="\t")

## Examine the first few rows and first 10 columns of the data
head(dat4train[,1:10])

## Declare the target class for the categories and the negative
positive="CD" #positive="Diseased"
negative="HC"
## Preassign colors of classes for visualization
col2use = c("gold","darkblue")

## Examine data properties
class(dat4train$cate)
## Make the category column a factor and negative class the reference
dat4train$cate<-factor(dat4train$cate, levels=c(negative,positive))
class(dat4train$cate)

```

## Exploring and visualize data

```{r dataXploration}
## How many OTUs (features) and samples are there? Hint: dim()

## Categories and counts of samples in each? Hint: table()

## Data distributions Hint: hist()

## Dimension Reduction using PCA Resource: https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html
pca_res <- prcomp(dat4train[,-1], scale. = TRUE)
autoplot(pca_res, data = dat4train, colour = 'cate', shape = FALSE, label.size = 3)

```

## Building a random forest classifier
### Hyperparameters:
Use the ?randomForest to examine defaults of the hyperparameters: ntree, mtry, sampsize, etc.
Out of Bag (OOB) error is the mean prediction error of left-out samples in trees. It is a different approach from cross validation, but they both report error estimates from left-out samples.
```{r rf}
## Read help on the function
?randomForest

## Set the number of trees to generate in the model
ntree=500

## Model tuning mytry: something to try, note the sample sizes are too small here
#tuneResult<-tuneRF(dat4train[,-1], dat4train$cate, stepFactor=1.5, ntreeTry=ntree)

## Set seed for report reproducibility here
seedNum=111;set.seed(seedNum)

## Build a random forest model
## Do you get the same model/result when you run randomForest or tuneRF multiple times on the same data without setting a seed?
rf1 = randomForest(cate ~ ., ntree = ntree, data = dat4train)
#rf1 = randomForest(cate ~ ., ntree = ntree, sampsize = c(21, 21), data = dat4train)


## Examine the model
print(rf1) 
plot(rf1)
 


```

### Examine the model outputs
Examine the infomation available about the model, what are they?
```{r rfmodel}
## The list of information you can retrieve from the random forest model
names(rf1)

## The confusion matrix
rf1$confusion

## The probabilities
head(rf1$votes)

## The predicted class per sample
head(rf1$predicted)

```


## Performance evaluation
### Practice own assessment: 
```{r ownAssessment}
### Obtaining confusion matrix ourselves
predvsRef <- data.frame(ref=dat4train$cate, pred= rf1$predicted)
table(predvsRef)

### Calculate sensitivity/recall yourselves


### Which samples were wrongly predicted?


```


### Confusion Matrix & other metrics
```{r perf_others}

### This is from OOB result
cmv <- confusionMatrix(data= rf1$predicted,  
                           reference=dat4train$cate,
                           positive=positive)

cmv

### What are the information provided
attributes(cmv)
cmv$overall
cmv$byClass

### One can also visualize the confusion matrix, but it's more informative for multi-class models.

``` 


### Generating receiver operating characteristic (ROC) curve & the area under curve (AUC)
```{r perf_AUC}
## Check the following parameters
rf.pred <- rf1$votes[,positive]
y.ref <- as.numeric(dat4train$cate==positive)
predROC0 <- prediction(rf.pred,y.ref)

aucV <- performance(predROC0, measure = "auc")@y.values[[1]]

plot(performance(predROC0, measure = "tpr", x.measure = "fpr"),main=paste("OOB: Pos =", positive))
abline(a=0, b= 1, col="darkgray",lty=2)	
text(0.7,0.2,labels=paste("AUC=",signif(aucV,2)))


```

## Variable importance
### Feature selection with CV 
Informing the number of top features w.r.t error through iterativelly reducing predictors in nested cross validation
```{r featSelectNum}

fresult <- rfcv(dat4train[,-1], dat4train[,1], cv.fold=5)
with(fresult, plot(n.var, error.cv, log="x", type="o", lwd=2))

```


### Variable importance plot

```{r varimp}
# Variable Importance plot
plottop = 20
varImpPlot(rf1, sort = T, n.var=plottop, main=paste("Top",plottop,"- Variable Importance"))

# Variable Importance 
var.imp <- data.frame(importance(rf1,type=2))
var.imp$Variables <- row.names(var.imp)  
head(var.imp[order(var.imp$MeanDecreaseGini,decreasing = T),])

var.imp <- var.imp[order(var.imp$MeanDecreaseGini,decreasing = T),] ## bug fix for plotting

```


### Key features w.r.t samples

```{r featVis}

hdat2plot <- t(data.matrix(dat4train[,head(var.imp$Variables, plottop)]))


heatmap.2(hdat2plot, trace="none", ColSideColors = col2use[dat4train$cate])
legend("topright", legend = levels(dat4train$cate), col = col2use, lty= 1, lwd = 5, cex=.7 )

```


## Predicting new samples
```{r predNew}

leftout <- "../data/leftout.txt"
newDat <- read.table(leftout, header = T, sep="\t")
newPred <- predict(rf1, newDat[,-1], type="response")

### prediction versus reference
data.frame(pred=newPred, ref=newDat$cate)

```

## Thoughts

Q1: What are the differences in the results from data #1 versus data #2? Why is there a descrepancy?

Q2: What would you comment about the model generated for data #3?

Q3: Describe the difference between the default model and sampsize-modified model on data #3.
